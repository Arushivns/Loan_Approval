{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BU3-VN8JqrNW"
      },
      "outputs": [],
      "source": [
        "#Load libraries\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import statsmodels.api as sm\n",
        "from scipy.stats import normaltest\n",
        "import statsmodels.formula.api as smf\n",
        "#plt.rcParams['figure.figsize']=(10,10)\n",
        "sns.set()\n",
        "sns.set(style=\"darkgrid\")\n",
        "import pylab as py\n",
        "\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, MinMaxScaler\n",
        "\n",
        "from sklearn.model_selection import KFold, cross_val_score, cross_val_predict\n",
        "from sklearn.feature_selection import SelectKBest, chi2, RFE\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import Lasso, Ridge\n",
        "\n",
        "from sklearn.compose import TransformedTargetRegressor\n",
        "\n",
        "#NN\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "\n",
        "from functools import partial\n",
        "from sklearn.metrics import accuracy_score\n",
        "#from skopt import space\n",
        "#from skopt import gp_minimize\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "abcNanaADGd8"
      },
      "outputs": [],
      "source": [
        "# EDA\n",
        "def UVA_numeric(data, var_group):\n",
        "  '''Â \n",
        "  Univariate_Analysis_numeric\n",
        "  takes a group of variables (INTEGER and FLOAT) and plot/print all the descriptives and properties along with KDE.\n",
        "\n",
        "  Runs a loop: calculate all the descriptives of i(th) variable and plot/print it\n",
        "  '''\n",
        "\n",
        "  size = len(var_group)\n",
        "  plt.figure(figsize = (4*size,3), dpi = 100)\n",
        "  \n",
        "  #looping for each variable\n",
        "  for j,i in enumerate(var_group):\n",
        "    \n",
        "    # calculating descriptives of variable\n",
        "    mini = data[i].min()\n",
        "    maxi = data[i].max()\n",
        "    ran = data[i].max()-data[i].min()\n",
        "    mean = data[i].mean()\n",
        "    median = data[i].median()\n",
        "    st_dev = data[i].std()\n",
        "    skew = data[i].skew()\n",
        "    kurt = data[i].kurtosis()\n",
        "\n",
        "    # calculating points of standard deviation\n",
        "    points = mean-st_dev, mean+st_dev\n",
        "\n",
        "    #Plotting the variable with every information\n",
        "    plt.subplot(1,size,j+1)\n",
        "    sns.kdeplot(data[i], shade=True)\n",
        "    sns.lineplot(points, [0,0], color = 'black', label = \"std_dev\")\n",
        "    sns.scatterplot([mini,maxi], [0,0], color = 'orange', label = \"min/max\")\n",
        "    sns.scatterplot([mean], [0], color = 'red', label = \"mean\")\n",
        "    sns.scatterplot([median], [0], color = 'blue', label = \"median\")\n",
        "    plt.xlabel('{}'.format(i), fontsize = 20)\n",
        "    plt.ylabel('density')\n",
        "    plt.title('std_dev = {}; kurtosis = {};\\nskew = {}; range = {}\\nmean = {}; median = {}'.format((round(points[0],2),round(points[1],2)),\n",
        "                                                                                                   round(kurt,2),\n",
        "                                                                                                   round(skew,2),\n",
        "                                                                                                   (round(mini,2),round(maxi,2),round(ran,2)),\n",
        "                                                                                                   round(mean,2),\n",
        "                                                                                                   round(median,2)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EZb1ctHyDCmA"
      },
      "outputs": [],
      "source": [
        "# Normality Check function\n",
        "\n",
        "def normality_check(data,var_group):\n",
        "    '''\n",
        "    Normality Check function\n",
        "    '''\n",
        "    size = len(var_group)\n",
        "    #plt.figure(figsize = (5*size,3), dpi = 100)\n",
        "    # qqplot\n",
        "    for j,i in enumerate(var_group):\n",
        "        skew=pd.DataFrame.skew(data[i])\n",
        "        print(\"Q_Q plot for \", i)\n",
        "        sm.qqplot(data[i],line='s')\n",
        "        py.show()   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dYGmcgAdC7Q1"
      },
      "outputs": [],
      "source": [
        "# Check outliers\n",
        "def UVA_outlier(data, var_group, include_outlier = True):\n",
        "  '''\n",
        "  Univariate_Analysis_outlier:\n",
        "  takes a group of variables (INTEGER and FLOAT) and plot/print boxplot and descriptives\\n\n",
        "  Runs a loop: calculate all the descriptives of i(th) variable and plot/print it \\n\\n\n",
        "\n",
        "  data : dataframe from which to plot from\\n\n",
        "  var_group : {list} type Group of Continuous variables\\n\n",
        "  include_outlier : {bool} whether to include outliers or not, default = True\\n\n",
        "  '''\n",
        "\n",
        "  size = len(var_group)\n",
        "  plt.figure(figsize = (5*size,4), dpi = 100)\n",
        "  \n",
        "  #looping for each variable\n",
        "  for j,i in enumerate(var_group):\n",
        "    \n",
        "    # calculating descriptives of variable\n",
        "    quant25 = data[i].quantile(0.25)\n",
        "    quant75 = data[i].quantile(0.75)\n",
        "    IQR = quant75 - quant25\n",
        "    med = data[i].median()\n",
        "    whis_low = med-(1.5*IQR)\n",
        "    whis_high = med+(1.5*IQR)\n",
        "\n",
        "    # Calculating Number of Outliers\n",
        "    outlier_high = len(data[i][data[i]>whis_high])\n",
        "    outlier_low = len(data[i][data[i]<whis_low])\n",
        "\n",
        "    if include_outlier == True:\n",
        "      print(include_outlier)\n",
        "      #Plotting the variable with every information\n",
        "      plt.subplot(1,size,j+1)\n",
        "      sns.boxplot(data[i], orient=\"v\")\n",
        "      plt.ylabel('{}'.format(i))\n",
        "      plt.title('With Outliers\\nIQR = {}; Median = {} \\n 2nd,3rd  quartile = {};\\n Outlier (low/high) = {} \\n'.format(\n",
        "                                                                                                   round(IQR,2),\n",
        "                                                                                                   round(med,2),\n",
        "                                                                                                   (round(quant25,2),round(quant75,2)),\n",
        "                                                                                                   (outlier_low,outlier_high)\n",
        "                                                                                                   ))\n",
        "      \n",
        "    else:\n",
        "      # replacing outliers with max/min whisker\n",
        "      data2 = data[var_group][:]\n",
        "      data2[i][data2[i]>whis_high] = whis_high+1\n",
        "      data2[i][data2[i]<whis_low] = whis_low-1\n",
        "      \n",
        "      # plotting without outliers\n",
        "      plt.subplot(1,size,j+1)\n",
        "      sns.boxplot(data2[i], orient=\"v\")\n",
        "      plt.ylabel('{}'.format(i))\n",
        "      plt.title('Without Outliers\\nIQR = {}; Median = {} \\n 2nd,3rd  quartile = {};\\n Outlier (low/high) = {} \\n'.format(\n",
        "                                                                                                   round(IQR,2),\n",
        "                                                                                                   round(med,2),\n",
        "                                                                                                   (round(quant25,2),round(quant75,2)),\n",
        "                                                                                                   (outlier_low,outlier_high)\n",
        "                                                                                                   ))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xwUFZwqIC2Vb"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FUY53Z5rCztS"
      },
      "outputs": [],
      "source": [
        "def UVA_category(data, var_group):\n",
        "\n",
        "  '''\n",
        "  Univariate_Analysis_categorical\n",
        "  takes a group of variables (category) and plot/print all the value_counts and barplot.\n",
        "  '''\n",
        "  # setting figure_size\n",
        "  size = len(var_group)\n",
        "  plt.figure(figsize = (7*size,5), dpi = 100)\n",
        "\n",
        "  # for every variable\n",
        "  for j,i in enumerate(var_group):\n",
        "    norm_count = data[i].value_counts(normalize = True)\n",
        "    n_uni = data[i].nunique()\n",
        "  #Plotting the variable with every information\n",
        "    plt.subplot(1,size,j+1)\n",
        "    sns.barplot(norm_count, norm_count.index , order = norm_count.index)\n",
        "    plt.xlabel('fraction/percent', fontsize = 20)\n",
        "    plt.ylabel('{}'.format(i), fontsize = 20)\n",
        "    plt.title('n_uniques = {} \\n value counts \\n {};'.format(n_uni,norm_count))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ue90IG0xCv4n"
      },
      "outputs": [],
      "source": [
        "#Fill NA function\n",
        "def fill_NAs (df):\n",
        "    '''\n",
        "    Replacing in categorical var. with most frequent values and in numerical with median. \n",
        "    '''\n",
        "    \n",
        "    cols_numerical = df.select_dtypes('float64').columns\n",
        "    for col in cols_numerical:\n",
        "        df[col].fillna((df[col].median()), inplace=True) \n",
        "    \n",
        "    cols_categorical = df.select_dtypes('category').columns\n",
        "    for col in cols_categorical:\n",
        "        df[col].fillna(df[col].mode()[0], inplace=True)\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pISylY39CtV5"
      },
      "outputs": [],
      "source": [
        "def BVA_categorical_plot(data, tar, cat):\n",
        "  '''\n",
        "  take data and two categorical variables,\n",
        "  calculates the chi2 significance between the two variables \n",
        "  and prints the result with countplot & CrossTab\n",
        "  '''\n",
        "  #isolating the variables\n",
        "  data = data[[cat,tar]][:]\n",
        "\n",
        "  #forming a crosstab\n",
        "  table = pd.crosstab(data[tar],data[cat],)\n",
        "  f_obs = np.array([table.iloc[0][:].values,\n",
        "                    table.iloc[1][:].values])\n",
        "\n",
        "  #performing chi2 test\n",
        "  from scipy.stats import chi2_contingency\n",
        "  chi, p, dof, expected = chi2_contingency(f_obs)\n",
        "  \n",
        "  #checking whether results are significant\n",
        "  if p<0.05:\n",
        "    sig = True\n",
        "  else:\n",
        "    sig = False\n",
        "\n",
        "  #plotting grouped plot\n",
        "  sns.countplot(x=cat, hue=tar, data=data)\n",
        "  plt.title(\"p-value = {}\\n difference significant? = {}\\n\".format(round(p,8),sig))\n",
        "\n",
        "  #plotting percent stacked bar plot\n",
        "  #sns.catplot(ax, kind='stacked')\n",
        "  ax1 = data.groupby(cat)[tar].value_counts(normalize=True).unstack()\n",
        "  ax1.plot(kind='bar', stacked='True',title=str(ax1))\n",
        "  int_level = data[cat].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vnJKOtoyCk51"
      },
      "outputs": [],
      "source": [
        "# Data Modeling I - Pipeline\n",
        "\n",
        "def PreProcessing_Pipeline(X,y):\n",
        "    '''\n",
        "    Perform transformation of data in the following steps: \n",
        "        - FIll NAs (Simple Imputer)\n",
        "        - One hot encoding to categorical variables\n",
        "        - Scaling using MinMaxScaler for numerical variables.\n",
        "                \n",
        "    Parameters\n",
        "    ----------\n",
        "    df : dataframe\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    df_transformed : dataframe with the transformations after the preprocessing. \n",
        "    \n",
        "    '''    \n",
        "    cols_numerical = X.select_dtypes('float64').columns     \n",
        "    cols_categorical = X.select_dtypes('category').columns   \n",
        "    cols_numerical = cols_numerical.tolist()\n",
        "    cols_categorical = cols_categorical.tolist(); #cols_categorical.remove('Loan_Status')\n",
        "    \n",
        "    # Define the pipeline steps (fill NAs, one hot, MinMaxScaler):\n",
        "       \n",
        "    numerical_preprocessor = make_pipeline(\n",
        "        SimpleImputer(strategy=\"median\"),\n",
        "        MinMaxScaler()    \n",
        "        )\n",
        "    \n",
        "    categorical_preprocessor = make_pipeline(\n",
        "        SimpleImputer(strategy=\"most_frequent\"),\n",
        "        OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
        "        )\n",
        "       \n",
        "    # define column transformer\n",
        "    preprocessing = ColumnTransformer([\n",
        "           ('numerical', numerical_preprocessor, cols_numerical),\n",
        "           ('categorical', categorical_preprocessor, cols_categorical),\n",
        "        ])\n",
        "    \n",
        "    # define pipeline\n",
        "    pipeline = Pipeline(steps=[('preprocessor', preprocessing)])\n",
        "       \n",
        "    # Fit and transform the data\n",
        "    pipeline.fit(X,y)\n",
        "    X_transformed = pipeline.named_steps['preprocessor'].transform(X)\n",
        "    \n",
        "    # Get the feature names after one-hot encoding\n",
        "    feature_names = preprocessing.get_feature_names_out()     \n",
        "      \n",
        "    # Create a dataframe with the selected features\n",
        "    X_transformed = pd.DataFrame(X_transformed, columns=feature_names)\n",
        "        \n",
        "    return preprocessing, X_transformed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X7yoP-6GCebI"
      },
      "outputs": [],
      "source": [
        "# Data Modeling II - Machine Learning Models\n",
        "\n",
        "\n",
        "def ML_Models(df):\n",
        "    '''\n",
        "    Function that trains different ML models doing different feature selection, hyperparameter tunning with a CV strategy.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df: dataframe with target variable (it must be df_transformed)\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    results : table with all results with\n",
        "        - the ML model, \n",
        "        - feature selection method, \n",
        "        - number of features selected for feature selection (k), \n",
        "        - accuracy, f1 and confusion to evaluate result of the prediction of the ML model\n",
        "        - best hyperparameters \n",
        "        \n",
        "    '''\n",
        "    #Define X, y\n",
        "    X = df.drop(columns=['Loan_Status', 'Loan_ID'])\n",
        "    y = df['Loan_Status'].replace({'Y': 1, 'N': 0})\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=0)\n",
        "    \n",
        "    #Pre processing the data\n",
        "    preprocessing, X_transformed = PreProcessing_Pipeline(X_train, y_train)\n",
        "    \n",
        "    # Define the models to evaluate\n",
        "    models = []\n",
        "    models.append(('SVC', SVC()))\n",
        "    models.append(('LR', LogisticRegression()))\n",
        "    models.append(('RF', RandomForestClassifier()))\n",
        "        \n",
        "    # Define the feature selection methods\n",
        "    methods = [('KBest(chi2)', SelectKBest(chi2))]#, ('RFE', RFE(estimator=LogisticRegression()))] \n",
        "    \n",
        "    # Define the number of features to consider\n",
        "    k_list = [5,10]\n",
        "    \n",
        "    results = []\n",
        "    \n",
        "    # Evaluate models with 10-fold cross-validation using different feature selection methods and k values\n",
        "    for name, model in models:\n",
        "        print(f\"Evaluating model {name}...\")\n",
        "        for method_name, method in methods:\n",
        "            print(f\"\\tEvaluating feature selection method {method_name}...\")\n",
        "            for k in k_list:\n",
        "                # Create the feature selection pipeline\n",
        "                feature_selection = Pipeline([('pp', preprocessing), ('fs', method), ('k', SelectKBest(k=k))])                \n",
        "                \n",
        "                # Create the model pipeline\n",
        "                model_pipeline = Pipeline([('fs', feature_selection), ('model', model)])\n",
        "                \n",
        "                # Define the cross-validation strategy\n",
        "                kfold = KFold(n_splits=10, shuffle=True, random_state=17)\n",
        "                \n",
        "                # Fit the GridSearchCV object to the data\n",
        "                if name == 'LR':\n",
        "                    param_grid = {'model__C': [0.1, 1, 2, 3, 4, 5, 10, 20, 30, 50, 75, 90, 100], 'model__penalty': ['l1', 'l2']}\n",
        "                    grid_search = GridSearchCV(model_pipeline, param_grid, cv=kfold)\n",
        "                elif name == 'RF':\n",
        "                    param_grid = {'model__n_estimators': [50, 100, 150],#, 200, 300, 400],\n",
        "                                  'model__max_depth': [3, 5],#, 7, 9, 11],\n",
        "                                  'model__min_samples_split': [2, 4],#, 6, 8],\n",
        "                                  'model__min_samples_leaf': [1, 3],#, 5],\n",
        "                                  'model__max_features': ['auto']}#], 'sqrt', 'log2']}\n",
        "                    grid_search = GridSearchCV(model_pipeline, param_grid, cv=kfold)\n",
        "                elif name == 'SVC':\n",
        "                    param_grid = {'model__C': [0.1, 1, 10, 100, 1000], \n",
        "                                  'model__gamma': [1, 0.1, 0.01, 0.001, 0.0001], \n",
        "                                  'model__kernel': ['linear', 'rbf']}\n",
        "                    grid_search = GridSearchCV(model_pipeline, param_grid, cv=kfold)\n",
        "                                \n",
        "                grid_search.fit(X_train, y_train)\n",
        "                \n",
        "                # Check if the best hyperparameters were found\n",
        "                if hasattr(grid_search, 'best_estimator_'):\n",
        "                    model_pipeline = grid_search.best_estimator_\n",
        "                \n",
        "                # Evaluate the model pipeline\n",
        "                y_pred = cross_val_predict(model_pipeline, X_test, y_test, cv=kfold)\n",
        "                \n",
        "                # Calculate accuracy and F1 score\n",
        "                accuracy = accuracy_score(y_test, y_pred)\n",
        "                f1 = f1_score(y_test, y_pred)\n",
        "                \n",
        "                # Calculate confusion matrix\n",
        "                conf_mat = confusion_matrix(y_test, y_pred)\n",
        "                \n",
        "                # Get the best hyperparameters for the model\n",
        "                best_params = grid_search.best_params_\n",
        "                \n",
        "                # Store the results\n",
        "                results.append({\n",
        "                    'model': name,\n",
        "                    'method': method_name,\n",
        "                    'k': k,\n",
        "                    'accuracy': accuracy,\n",
        "                    #'selected_features': selected_features,\n",
        "                    'f1': f1,\n",
        "                    'confusion_matrix': conf_mat,\n",
        "                    'best_params': best_params\n",
        "                })\n",
        "                print(f\"Results for {name} model with {method_name} feature selection and k={k}:\")\n",
        "                print(results[-1]) # Print the last result added to the list\n",
        "        \n",
        "        print(f\"{name} model finished.\")                \n",
        "    # Convert the results to a DataFrame\n",
        "    results = pd.DataFrame(results)\n",
        "    \n",
        "    print(\"All models finished.\")\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WSNTVmhUCarE"
      },
      "outputs": [],
      "source": [
        "# Extra - Create manual features\n",
        "\n",
        "def create_manual_features (df):\n",
        "    '''\n",
        "    New variables are added to the dataframe df. \n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df: dataframe (not transformed)\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    dff: dataframe with new features.\n",
        "    \n",
        "    '''\n",
        "    dff = fill_NAs(df)\n",
        "    encoder = LabelEncoder()\n",
        "    #cols_categorical = dff.select_dtypes('category').columns\n",
        "    #cols_categorical = cols_categorical.tolist(); cols_categorical.remove('Loan_Status')\n",
        "    #for cat in cols_categorical:\n",
        "    #    dff[cat] = encoder.fit_transform(dff[cat])\n",
        "    \n",
        "    dff['Loan_Amount_Term'] = dff['Loan_Amount_Term'].astype('int64')\n",
        "    dff = dff[dff['Loan_Amount_Term'] > 0]\n",
        "    dff['MF_Total_Income'] = dff['ApplicantIncome'] + dff['CoapplicantIncome']\n",
        "    dff['MF_LoanAmount_log'] = np.log(dff['LoanAmount']); dff['MF_ApplicantIncome_log'] = np.log(dff['ApplicantIncome']);  dff['MF_Total_Income_log'] = np.log(dff['MF_Total_Income'])\n",
        "    dff['MF_LoanAmount/AppIncome'] = dff['MF_LoanAmount_log'] / dff['MF_ApplicantIncome_log']\n",
        "    dff['MF_LoanAmount/TotalInc'] = dff['MF_LoanAmount_log'] / dff['MF_Total_Income_log']\n",
        "    dff['MF_LoanAmount_ratio'] = dff['LoanAmount']/dff['MF_Total_Income']\n",
        "    dff['MF_Income_education'] = dff['MF_ApplicantIncome_log'] * encoder.fit_transform(dff['Education'])\n",
        "    dff['MF_Income_employed'] = dff['MF_ApplicantIncome_log'] * encoder.fit_transform(dff['Self_Employed'])\n",
        "    dff['MF_LoanAmount_monthly'] = dff['LoanAmount']/dff['Loan_Amount_Term']\n",
        "       \n",
        "    return dff"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ZK9JT57CVxu"
      },
      "outputs": [],
      "source": [
        "# Extra - Create auto features\n",
        "\n",
        "def create_new_features(df, degree=2): #, n_bins=5\n",
        "    '''\n",
        "    Create a lot of new variables using polynomial, exponential, binning and sqrt transformation \n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    df : dataframe with or without transformation\n",
        "    degree : degree of the polynomial transformation, by default 2\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    new_df : dataframe with new variables\n",
        "        \n",
        "    '''\n",
        "    dff = fill_NAs(df)\n",
        "\n",
        "    # Create polynomial features\n",
        "    new_df_poly = pd.DataFrame()\n",
        "    #new_df_sqrt = pd.DataFrame()\n",
        "    #new_df_exp = pd.DataFrame()\n",
        "    #new_df_bin = pd.DataFrame()\n",
        "    \n",
        "    #Encoding categorical variables\n",
        "    encoder = LabelEncoder()\n",
        "    cols_categorical = dff.select_dtypes('category').columns\n",
        "    cols_categorical = cols_categorical.tolist(); cols_categorical.remove('Loan_Status')\n",
        "    for cat in cols_categorical:\n",
        "        dff[cat] = encoder.fit_transform(dff[cat])\n",
        "    \n",
        "    X = dff.drop(['Loan_Status', 'Loan_ID'], axis = 1)\n",
        "    \n",
        "    poly = PolynomialFeatures(degree)\n",
        "    X_poly = poly.fit_transform(X)\n",
        "    new_df_poly = pd.DataFrame(X_poly, columns=poly.get_feature_names_out(X.columns))\n",
        "  \n",
        "    #for col in X.select_dtypes(include = [np.float64]).columns:\n",
        "        #new_df_sqrt['reciprocal_' + col] = 1/df[col]\n",
        "        #new_df_exp['exp_' + col] = np.exp(df[col])\n",
        "        #bins = np.linspace(df[col].min(), df[col].max(), n_bins + 1)\n",
        "        #new_df_bin['bin_' + col] = pd.cut(df[col], bins)\n",
        "        #new_df_bin['bin_' + col] = pd.get_dummies(new_df_bin['bin_' + col])    \n",
        "    \n",
        "    new_df = pd.concat([new_df_poly.reset_index(drop=True), dff], axis=1)\n",
        "    #Remove duplicates\n",
        "    new_df = new_df.loc[:,~new_df.T.duplicated(keep='first')]\n",
        "    #Remove those variables with var=0\n",
        "    # Get the variance of each column in the DataFrame\n",
        "    variances = new_df.var()\n",
        "    # Get the names of the columns with zero variance\n",
        "    zero_variance_vars = variances[variances == 0].index.tolist()\n",
        "    # Drop the columns with zero variance\n",
        "    new_df.drop(columns=zero_variance_vars, inplace=True)\n",
        "    #new_df.dropna(inplace=True)\n",
        "    return new_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iaHgaDVDCRtU"
      },
      "outputs": [],
      "source": [
        "def NN_Model (df):\n",
        "\n",
        "    '''\n",
        "    Train a simple neural network \n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    df: dataframe with target variable (it must be df_transformed)\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    The test accuracy, f1 score, and confusion matrix of the model\n",
        "    \n",
        "    '''\n",
        "        \n",
        "    #Define X, y\n",
        "    X = df.drop(['Loan_Status'], axis = 1)\n",
        "    y = df['Loan_Status'].replace({'Y': 1, 'N': 0})\n",
        "    \n",
        "    preprocessing, X_transformed = PreProcessing_Pipeline(X,y)\n",
        "    \n",
        "    # Train and test split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_transformed, y, test_size=0.2, random_state=0)\n",
        "\n",
        "    # Initialize the model\n",
        "    model = Sequential()    \n",
        "    # Add the first hidden layer with 128 neurons and a ReLU activation function\n",
        "    model.add(Dense(128, activation='relu', input_shape=(X_train.shape[1],)))\n",
        "    # Add the second hidden layer with 64 neurons and a ReLU activation function\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    # Add the output layer with 1 neuron and a sigmoid activation function for binary classification\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    # Compile the model\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    # Fit the model to the training data\n",
        "    model.fit(X_train, y_train, epochs=20, batch_size=32)\n",
        "    # Predict the labels for the test data\n",
        "    y_pred = (model.predict(X_test) > 0.5).astype(int)\n",
        "    # Calculate the test accuracy\n",
        "    test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
        "    print(f'Test Accuracy: {test_accuracy}')\n",
        "    # Calculate the F1 score\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "    print(f'F1 score: {f1}')\n",
        "    # Calculate the confusion matrix\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    print(f'Confusion matrix:\\n{cm}')\n",
        "    return test_accuracy, f1, cm    "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
